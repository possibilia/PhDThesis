{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0077401b-9a61-4d01-a092-f1695d5f2e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "with open('config.json') as infile:\n",
    "    config = json.load(infile)\n",
    "    \n",
    "repo = config['laptop']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8cb138-7cfb-450b-b87d-b8265f2373a9",
   "metadata": {},
   "source": [
    "# Model checking plan data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18da77ec-2e54-4a87-9b63-9638fb3d2727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_plan_data(run):\n",
    "    data = []\n",
    "    file = open(repo+'/{}/autoctrl.txt'.format(run), 'r')\n",
    "    \n",
    "    for line in file:\n",
    "        if line[0:3] == 'DEL':\n",
    "            data.append(float(re.findall('\\d+\\.\\d+|\\d+', line)[0]))\n",
    "            \n",
    "    file.close()\n",
    "    data.append(len(data))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def add_plan_to_run_data(n_runs):\n",
    "    count = 0\n",
    "    for run in os.listdir(repo):    \n",
    "        with open(repo+'/{}/{}.json'.format(run, run), 'r') as infile:\n",
    "            run_data = json.load(infile)\n",
    "\n",
    "        log_data = extract_plan_data(run)\n",
    "        \n",
    "        for i in range(10):\n",
    "            if i < len(log_data[:-1]):\n",
    "                run_data['run{}'.format(i)] = log_data[i]\n",
    "            else:\n",
    "                run_data['run{}'.format(i)] = None\n",
    "                \n",
    "        run_data['n_plans'] = log_data[-1]\n",
    "        with open(repo+'/{}/{}.json'.format(run, run), 'w') as outfile: \n",
    "            json.dump(run_data, outfile)\n",
    "            \n",
    "        count += 1\n",
    "      \n",
    "    try:\n",
    "        assert count == n_runs\n",
    "    except AssertionError:\n",
    "        print('FAILED: count = {}'.format(count))\n",
    "\n",
    "add_plan_to_run_data(n_runs=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2502c0d6-48b9-46fe-a841-4fb215d267af",
   "metadata": {},
   "source": [
    "# Process resource usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5009c9ec-7ae2-400d-b75a-25019c72603b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_process_memory(run):\n",
    "    swap = []\n",
    "    physical = []\n",
    "    process = []\n",
    "    \n",
    "    file = open(repo+'/{}/usage.txt'.format(run), 'r')\n",
    "    \n",
    "    for line in file:\n",
    "        if line.split(':')[0] == 'MiB Mem ':\n",
    "            physical.append(line.split(':')[-1].strip())\n",
    "        elif line.split(':')[0] == 'MiB Swap':\n",
    "            swap.append(line.split(':')[-1].strip())\n",
    "        elif line.strip()[-8:] == 'autoctrl':\n",
    "            process.append(line)\n",
    "\n",
    "    file.close()\n",
    "\n",
    "    # physical data\n",
    "    physical_ = []\n",
    "    physical = [x.split(',') for x in physical]\n",
    "    for row in physical:\n",
    "        physical_.append([float(x.strip()[:4]) for x in row])\n",
    "\n",
    "    # swap data\n",
    "    swap_ = []\n",
    "    swap = [x.split(',') for x in swap]\n",
    "    for row in swap:   \n",
    "        swap_.append([x.strip() for x in row])\n",
    "\n",
    "    swap__ = []\n",
    "    for row in swap_:\n",
    "        swap__.append([float(x.split()[0]) for x in row])\n",
    "\n",
    "    # process data\n",
    "    process_ = []\n",
    "    for row in process:   \n",
    "        process_.append(row.strip()[9:-8].strip().split(' '))\n",
    "\n",
    "    process__ = []\n",
    "    for row in process_:\n",
    "        process__.append([x for x in row if x != ''])\n",
    "\n",
    "    for i in range(len(process__)):    \n",
    "        for j in range(len(process__[i])):\n",
    "            if ':' in process__[i][j]:\n",
    "                process__[i][j] = process__[i][j][-5:]\n",
    "                \n",
    "            try:\n",
    "                process__[i][j] = float(process__[i][j])\n",
    "            except ValueError:\n",
    "                pass\n",
    "                \n",
    "    phy_cols = ['mem_total', 'mem_free', 'mem_used', 'cache']\n",
    "    phy = pd.DataFrame(data=physical_, columns=phy_cols)\n",
    "\n",
    "    swp_cols = ['swap_total', 'swap_free', 'swap_used']\n",
    "    swp = pd.DataFrame(data=swap__, columns=swp_cols)\n",
    "\n",
    "    proc_cols = ['pr', 'ni', 'virt', 'res', 'shr', 'status', \n",
    "                 'cpu_perc', 'mem_perc', 'cpu_time']\n",
    "    proc = pd.DataFrame(data=process__, columns=proc_cols)\n",
    "\n",
    "    df = pd.merge(proc, phy, how='inner', left_index=True, right_index=True)\n",
    "    df = pd.merge(df, swp, how='inner', left_index=True, right_index=True)\n",
    "    df['process_uptime'] = 0.001 * df.index\n",
    "    df = df[df.mem_perc > 0]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_process_to_run_data(n_runs):\n",
    "    count = 0\n",
    "    for run in os.listdir(repo):    \n",
    "        with open(repo+'/{}/{}.json'.format(run, run), 'r') as infile:\n",
    "            run_data = json.load(infile)\n",
    "\n",
    "        df = _extract_process_memory(run)\n",
    "        df.to_csv(repo+'/{}/{}usage.csv'.format(run, run))\n",
    "\n",
    "        run_data['virt_min'] = df.virt.min()\n",
    "        run_data['virt_max'] = df.virt.max()\n",
    "        run_data['virt_sum'] = df.virt.sum()\n",
    "        run_data['res_min'] = df.res.min()\n",
    "        run_data['res_max'] = df.res.max()\n",
    "        run_data['res_sum'] = df.res.sum()\n",
    "        run_data['shr_min'] = df.shr.min()\n",
    "        run_data['shr_max'] = df.shr.max()\n",
    "        run_data['shr_sum'] = df.shr.sum()\n",
    "        run_data['mem_perc_min'] = df.mem_perc.min()\n",
    "        run_data['mem_perc_max'] = df.mem_perc.max()\n",
    "        run_data['mem_perc_mean'] = df.mem_perc.mean()\n",
    "        run_data['mem_perc_median'] = df.mem_perc.median()\n",
    "        run_data['mem_total_min'] = df.mem_total.min()\n",
    "        run_data['mem_total_max'] = df.mem_total.max()\n",
    "        run_data['mem_total_mean'] = df.mem_perc.mean()\n",
    "        run_data['mem_total_median'] = df.mem_perc.median()\n",
    "        run_data['mem_free_min'] = df.mem_total.min()\n",
    "        run_data['mem_free_max'] = df.mem_total.max()\n",
    "        run_data['mem_free_mean'] = df.mem_free.mean()\n",
    "        run_data['mem_free_median'] = df.mem_free.median()\n",
    "        run_data['mem_used_min'] = df.mem_used.min()\n",
    "        run_data['mem_used_max'] = df.mem_used.max()\n",
    "        run_data['mem_used_mean'] = df.mem_used.mean()\n",
    "        run_data['mem_used_median'] = df.mem_used.median()\n",
    "        run_data['cache_min'] = df.cache.min()\n",
    "        run_data['cache_max'] = df.cache.max()\n",
    "        run_data['cache_mean'] = df.cache.mean()\n",
    "        run_data['cache_median'] = df.cache.median()\n",
    "        run_data['swap_total_min'] = df.swap_total.min()\n",
    "        run_data['swap_total_max'] = df.swap_total.max()\n",
    "        run_data['swap_total_mean'] = df.swap_total.mean()\n",
    "        run_data['swap_total_median'] = df.swap_total.median()\n",
    "        run_data['swap_free_min'] = df.swap_free.min()\n",
    "        run_data['swap_free_max'] = df.swap_free.max()\n",
    "        run_data['swap_free_mean'] = df.swap_free.mean()\n",
    "        run_data['swap_free_median'] = df.swap_free.median()\n",
    "        run_data['swap_used_min'] = df.swap_used.min()\n",
    "        run_data['swap_used_max'] = df.swap_used.max()\n",
    "        run_data['swap_used_mean'] = df.swap_used.mean()\n",
    "        run_data['swap_used_median'] = df.swap_used.median()\n",
    "        run_data['process_uptime'] = df.process_uptime.max()\n",
    "\n",
    "        with open(repo+'/{}/{}.json'.format(run, run), 'w') as outfile: \n",
    "            json.dump(run_data, outfile)\n",
    "\n",
    "        count += 1\n",
    "    try:\n",
    "        assert count == n_runs\n",
    "    except AssertionError:\n",
    "        print('FAILED: count = {}'.format(count))\n",
    "\n",
    "add_process_to_run_data(n_runs=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0865ac-2771-4597-a37a-eca8f42f50d6",
   "metadata": {},
   "source": [
    "# Model checking resource usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "acf51542-4ddb-4815-9ad4-d65c9eb47655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_stack_data(data):\n",
    "    result = []\n",
    "    \n",
    "    for item in data:\n",
    "        if item[:3] != 'SET':\n",
    "            result.append(item.strip())\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    result = result[2:-2]\n",
    "    # get stack size data here\n",
    "    result = [int(x[-1]) for x in result]\n",
    "            \n",
    "    return result\n",
    "    \n",
    "def _extract_set_data(data):\n",
    "    result = []\n",
    "    \n",
    "    for item in reversed(data[:-5]):\n",
    "        if item[:3] != 'SET':\n",
    "            try:\n",
    "                result.append(int(item.strip()))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    return result\n",
    "    \n",
    "def extract_performance_data(run, algo):\n",
    "    perf = []\n",
    "    \n",
    "    for file in os.listdir(repo+'\\{}'.format(run)):\n",
    "        if file[:3] == algo:\n",
    "            with open(repo+'\\{}\\{}'.format(run, file)) as infile:\n",
    "                perf.append(infile.readlines())\n",
    "         \n",
    "    set_data = []\n",
    "    stack_data = []\n",
    "    \n",
    "    for data in perf:\n",
    "        stack_data += _extract_stack_data(data)\n",
    "        set_data += _extract_set_data(data)\n",
    "    \n",
    "    return stack_data, set_data\n",
    "\n",
    "def add_performance_to_run_data(n_runs, algo):\n",
    "    count = 0\n",
    "    for run in os.listdir(repo):    \n",
    "        with open(repo+'/{}/{}.json'.format(run, run), 'r') as infile:\n",
    "            run_data = json.load(infile)\n",
    "\n",
    "        stack_data, set_data = extract_performance_data(run, algo)\n",
    "        run_data['{}_state_size'.format(algo)] = 4\n",
    "        run_data['{}_n_states'.format(algo)] = 15\n",
    "        run_data['{}_adj_list_compile'.format(algo)] = 184\n",
    "        run_data['{}_stack_compile'.format(algo)] = 12\n",
    "        run_data['{}_set_compile'.format(algo)] = 24\n",
    "        \n",
    "        if len(stack_data) > 0:\n",
    "            # max stack size and number of steps\n",
    "            run_data['{}_max_stack_capacity'.format(algo)] = max(stack_data)\n",
    "        else:\n",
    "            run_data['{}_max_stack_capacity'.format(algo)] = None\n",
    "            \n",
    "        if len(set_data) > 0:\n",
    "            run_data['{}_max_set_size'.format(algo)] = max(set_data)\n",
    "        else:\n",
    "            run_data['{}_max_set_size'.format(algo)] = None\n",
    "            \n",
    "        with open(repo+'/{}/{}.json'.format(run, run), 'w') as outfile: \n",
    "            json.dump(run_data, outfile)\n",
    "            \n",
    "        count += 1\n",
    "      \n",
    "    try:\n",
    "        assert count == n_runs\n",
    "    except AssertionError:\n",
    "        print('FAILED: count = {}'.format(count))\n",
    "\n",
    "add_performance_to_run_data(n_runs=90, algo='dfs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b6c33d-5797-4d19-bbc3-6f157c68854c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "624067bf-a8aa-41ca-9c31-ad809175cb18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "repo = '<path to repo>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75882e61-4f7e-43d1-a610-e9e681b9ce44",
   "metadata": {},
   "source": [
    "# Model checking plan data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "925ed9c0-5e3f-418d-b295-cc49428ff324",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_plan_data(run):\n",
    "    data = []\n",
    "    file = open(repo+'/{}/autoctrl.txt'.format(run), 'r')\n",
    "    \n",
    "    for line in file:\n",
    "        if line.split(':')[0] == 'ACCEPT':\n",
    "            data.append(line.split(':')[-1].strip())\n",
    "        elif line.split(':')[0] == 'PATH':\n",
    "            data.append(line.split(':')[-1].strip())\n",
    "        elif line.split(':')[0] == 'PLAN':\n",
    "            data.append(line.split(':')[-1].strip())\n",
    "        elif line.strip()[-2:] == 'ms':\n",
    "            d = line.split('!')[1].strip()[:-2]\n",
    "            data.append(float(d))\n",
    "            \n",
    "    file.close()\n",
    "    \n",
    "    if len(data) == 0:\n",
    "        data = [None] * 8\n",
    "    \n",
    "    if len(data) == 4:\n",
    "        data += [None] * 4\n",
    "\n",
    "    return data\n",
    "            \n",
    "def add_plan_to_run_data(n_runs):\n",
    "    count = 0\n",
    "    for run in os.listdir(repo):    \n",
    "        with open(repo+'/{}/{}.json'.format(run, run), 'r') as infile:\n",
    "            run_data = json.load(infile)\n",
    "\n",
    "        log_data = extract_plan_data(run)\n",
    "        run_data['accept1'] = log_data[0]\n",
    "        run_data['path1'] = log_data[1]\n",
    "        run_data['plan1'] = log_data[2]\n",
    "        if (log_data[2] != None):\n",
    "            run_data['steps1'] = int(len(log_data[2].split('->')[:-1]))\n",
    "        else:\n",
    "            run_data['steps1'] = log_data[2]\n",
    "        run_data['latency1'] = log_data[3]\n",
    "        run_data['accept2'] = log_data[4]\n",
    "        run_data['path2'] = log_data[5]\n",
    "        run_data['plan2'] = log_data[6]\n",
    "        if (log_data[6] != None):\n",
    "            run_data['steps2'] = int(len(log_data[6].split('->')[:-1]))\n",
    "        else:\n",
    "            run_data['steps2'] = log_data[6]\n",
    "        run_data['latency2'] = log_data[7]\n",
    "\n",
    "        with open(repo+'/{}/{}.json'.format(run, run), 'w') as outfile: \n",
    "            json.dump(run_data, outfile)\n",
    "            \n",
    "        count += 1\n",
    "      \n",
    "    try:\n",
    "        assert count == n_runs\n",
    "    except AssertionError:\n",
    "        print('FAILED: count = {}'.format(count))\n",
    "    \n",
    "add_plan_to_run_data(n_runs=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7575676-10e6-49b6-ae74-1d86b11e5eb5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model checking resource usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97776ff0-3e39-4098-9d93-3a90188d768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_stack_data(data):\n",
    "    result = []\n",
    "    \n",
    "    for item in data:\n",
    "        if item[:3] != 'SET':\n",
    "            result.append(item.strip())\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    result = result[2:-2]\n",
    "    # get stack size data here\n",
    "    result = [int(x[-1]) for x in result]\n",
    "            \n",
    "    return result\n",
    "    \n",
    "def _extract_set_data(data):\n",
    "    result = []\n",
    "    \n",
    "    for item in reversed(data[:-5]):\n",
    "        if item[:3] != 'SET':\n",
    "            try:\n",
    "                result.append(int(item.strip()))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    return result\n",
    "    \n",
    "def extract_performance_data(run):\n",
    "    perf = []\n",
    "    \n",
    "    for file in os.listdir(repo+'\\{}'.format(run)):\n",
    "        if file[:3] == 'dfs':\n",
    "            with open(repo+'\\{}\\{}'.format(run, file)) as infile:\n",
    "                perf.append(infile.readlines())\n",
    "         \n",
    "    set_data = []\n",
    "    stack_data = []\n",
    "    \n",
    "    for data in perf:\n",
    "        stack_data += _extract_stack_data(data)\n",
    "        set_data += _extract_set_data(data)\n",
    "    \n",
    "    return stack_data, set_data\n",
    "\n",
    "def add_performance_to_run_data(n_runs):\n",
    "    count = 0\n",
    "    for run in os.listdir(repo):    \n",
    "        with open(repo+'/{}/{}.json'.format(run, run), 'r') as infile:\n",
    "            run_data = json.load(infile)\n",
    "\n",
    "        stack_data, set_data = extract_performance_data(run)\n",
    "        run_data['state_size'] = 4\n",
    "        run_data['n_states'] = 15\n",
    "        run_data['adj_list_compile'] = 184\n",
    "        run_data['stack_compile'] = 12\n",
    "        run_data['set_compile'] = 24\n",
    "        \n",
    "        if len(stack_data) > 0:\n",
    "            # max stack size and number of steps\n",
    "            run_data['max_stack_capacity'] = max(stack_data)\n",
    "        else:\n",
    "            run_data['max_stack_capacity'] = None\n",
    "            \n",
    "        if len(set_data) > 0:\n",
    "            run_data['max_set_size'] = max(set_data)\n",
    "        else:\n",
    "            run_data['max_set_size'] = None\n",
    "            \n",
    "        with open(repo+'/{}/{}.json'.format(run, run), 'w') as outfile: \n",
    "            json.dump(run_data, outfile)\n",
    "            \n",
    "        count += 1\n",
    "      \n",
    "    try:\n",
    "        assert count == n_runs\n",
    "    except AssertionError:\n",
    "        print('FAILED: count = {}'.format(count))\n",
    "\n",
    "add_performance_to_run_data(n_runs=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124f0215-731d-44b2-bc2b-3e6739869c22",
   "metadata": {},
   "source": [
    "# Process resource usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72e69058-185e-4039-a558-3621d9b2001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_process_memory(run):\n",
    "    swap = []\n",
    "    physical = []\n",
    "    process = []\n",
    "    \n",
    "    file = open(repo+'/{}/usage.txt'.format(run), 'r')\n",
    "    \n",
    "    for line in file:\n",
    "        if line.split(':')[0] == 'MiB Mem ':\n",
    "            physical.append(line.split(':')[-1].strip())\n",
    "        elif line.split(':')[0] == 'MiB Swap':\n",
    "            swap.append(line.split(':')[-1].strip())\n",
    "        elif line.strip()[-8:] == 'autoctrl':\n",
    "            process.append(line)\n",
    "\n",
    "    file.close()\n",
    "\n",
    "    # physical data\n",
    "    physical_ = []\n",
    "    physical = [x.split(',') for x in physical]\n",
    "    for row in physical:\n",
    "        physical_.append([float(x.strip()[:4]) for x in row])\n",
    "\n",
    "    # swap data\n",
    "    swap_ = []\n",
    "    swap = [x.split(',') for x in swap]\n",
    "    for row in swap:   \n",
    "        swap_.append([x.strip() for x in row])\n",
    "\n",
    "    swap__ = []\n",
    "    for row in swap_:\n",
    "        swap__.append([float(x.split()[0]) for x in row])\n",
    "\n",
    "    # process data\n",
    "    process_ = []\n",
    "    for row in process:   \n",
    "        process_.append(row.strip()[9:-8].strip().split(' '))\n",
    "\n",
    "    process__ = []\n",
    "    for row in process_:\n",
    "        process__.append([x for x in row if x != ''])\n",
    "\n",
    "    for i in range(len(process__)):    \n",
    "        for j in range(len(process__[i])):\n",
    "            if ':' in process__[i][j]:\n",
    "                process__[i][j] = process__[i][j][-5:]\n",
    "                \n",
    "            try:\n",
    "                process__[i][j] = float(process__[i][j])\n",
    "            except ValueError:\n",
    "                pass\n",
    "                \n",
    "    phy_cols = ['mem_total', 'mem_free', 'mem_used', 'cache']\n",
    "    phy = pd.DataFrame(data=physical_, columns=phy_cols)\n",
    "\n",
    "    swp_cols = ['swap_total', 'swap_free', 'swap_used']\n",
    "    swp = pd.DataFrame(data=swap__, columns=swp_cols)\n",
    "\n",
    "    proc_cols = ['pr', 'ni', 'virt', 'res', 'shr', 'status', \n",
    "                 'cpu_perc', 'mem_perc', 'cpu_time']\n",
    "    proc = pd.DataFrame(data=process__, columns=proc_cols)\n",
    "\n",
    "    df = pd.merge(proc, phy, how='inner', left_index=True, right_index=True)\n",
    "    df = pd.merge(df, swp, how='inner', left_index=True, right_index=True)\n",
    "    df['process_uptime'] = 0.001 * df.index\n",
    "    df = df[df.mem_perc > 0]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_process_to_run_data(n_runs):\n",
    "    count = 0\n",
    "    for run in os.listdir(repo):    \n",
    "        with open(repo+'/{}/{}.json'.format(run, run), 'r') as infile:\n",
    "            run_data = json.load(infile)\n",
    "\n",
    "        df = _extract_process_memory(run)\n",
    "        df.to_csv(repo+'/{}/{}usage.csv'.format(run, run))\n",
    "\n",
    "        run_data['virt_min'] = df.virt.min()\n",
    "        run_data['virt_max'] = df.virt.max()\n",
    "        run_data['res_min'] = df.res.max()\n",
    "        run_data['res_max'] = df.res.max()\n",
    "        run_data['shr_min'] = df.shr.min()\n",
    "        run_data['shr_max'] = df.shr.max()\n",
    "        run_data['mem_perc_min'] = df.mem_perc.min()\n",
    "        run_data['mem_total_min'] = df.mem_total.min()\n",
    "        run_data['mem_perc_max'] = df.mem_perc.max()\n",
    "        run_data['mem_total_max'] = df.mem_total.max()\n",
    "        run_data['mem_free_min'] = df.mem_total.min()\n",
    "        run_data['mem_free_max'] = df.mem_total.max()\n",
    "        run_data['mem_used_min'] = df.mem_used.min()\n",
    "        run_data['mem_used_max'] = df.mem_used.max()\n",
    "        run_data['cache_min'] = df.cache.min()\n",
    "        run_data['cache_max'] = df.cache.max()\n",
    "        run_data['swap_total_min'] = df.swap_total.min()\n",
    "        run_data['swap_total_max'] = df.swap_total.max()\n",
    "        run_data['swap_free_min'] = df.swap_free.min()\n",
    "        run_data['swap_free_max'] = df.swap_free.max()\n",
    "        run_data['swap_used_min'] = df.swap_used.min()\n",
    "        run_data['swap_used_max'] = df.swap_used.max()\n",
    "        run_data['process_uptime'] = df.process_uptime.max()\n",
    "\n",
    "        with open(repo+'/{}/{}.json'.format(run, run), 'w') as outfile: \n",
    "            json.dump(run_data, outfile)\n",
    "\n",
    "        count += 1\n",
    "    try:\n",
    "        assert count == n_runs\n",
    "    except AssertionError:\n",
    "        print('FAILED: count = {}'.format(count))\n",
    "\n",
    "add_process_to_run_data(n_runs=90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "624067bf-a8aa-41ca-9c31-ad809175cb18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import statistics\n",
    "\n",
    "# with open('config.json') as infile:\n",
    "#     config = json.load(infile)\n",
    "    \n",
    "# repo = config['desktop']\n",
    "\n",
    "repo = 'C:/Users/possi/OneDrive - University of Glasgow/data/FMAS2024_RAW_rev'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75882e61-4f7e-43d1-a610-e9e681b9ce44",
   "metadata": {},
   "source": [
    "# Model checking plan data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "925ed9c0-5e3f-418d-b295-cc49428ff324",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '0 1', 'left -> default', 7.293, '14', '0 2 13', 'right -> right -> default', 6.315, '14', '0 1 13', 'left -> left -> default', 9.899, '14', '0 1 13', 'left -> left -> default', 7.551, '14', '0 1 13', 'left -> left -> default', 7.155, '7 11', '0 1 3 5', 'left -> straight -> right -> default', 14.861, '14', '0 2 13', 'right -> right -> default', 5.39, '14', '0 2 13', 'right -> right -> default', 6.946, '14', '0 2 13', 'right -> right -> default', 7.662, '14', '0 1 13', 'left -> left -> default', 9.336, '14', '0 2 13', 'right -> right -> default', 5.975, '14', '0 2 13', 'right -> right -> default', 5.915, '8 12', '0 2 4 10', 'right -> straight -> right -> default', 5.456, '12', '0 2 4 10', 'right -> straight -> right -> default', 10.296, '8 12', '0 2 4 6', 'right -> straight -> left -> default', 7.352, '11', '0 1 3 9', 'left -> straight -> left -> default', 10.183, '4', '0 2', 'right -> default', 3.508, '14', '0 2 13', 'right -> right -> default', 8.005, '14', '0 1 13', 'left -> left -> default', 9.889, '7 11', '0 1 3 5', 'left -> straight -> right -> default', 18.089, '4', '0 2', 'right -> default', 9.147, '3', '0 1', 'left -> default', 4.603, '3', '0 1', 'left -> default', 4.278, '3', '0 1', 'left -> default', 7.651, '14', '0 1 13', 'left -> left -> default', 12.008, '4', '0 2', 'right -> default', 5.582, '14', '0 2 13', 'right -> right -> default', 5.872, '14', '0 1 13', 'left -> left -> default', 7.037, '4', '0 2', 'right -> default', 5.464, '12', '0 2 4 10', 'right -> straight -> right -> default', 9.473, '14', '0 2 13', 'right -> right -> default', 6.942, '14', '0 1 13', 'left -> left -> default', 6.634, '3', '0 1', 'left -> default', 5.58, '7 11', '0 1 3 5', 'left -> straight -> right -> default', 8.842, '14', '0 1 13', 'left -> left -> default', 12.15, '3', '0 1', 'left -> default', 7.349, '3', '0 1', 'left -> default', 6.698, '4', '0 2', 'right -> default', 4.658, '14', '0 1 13', 'left -> left -> default', 6.417, '14', '0 1 13', 'left -> left -> default', 9.113, '14', '0 1 13', 'left -> left -> default', 6.649, '3', '0 1', 'left -> default', 6.42, '3', '0 1', 'left -> default', 3.929, '3', '0 1', 'left -> default', 8.4, '4', '0 2', 'right -> default', 3.213, '11', '0 1 3 9', 'left -> straight -> left -> default', 11.457]\n"
     ]
    }
   ],
   "source": [
    "def extract_plan_data(run):\n",
    "    data = []\n",
    "    file = open(repo+'/{}/autoctrl.txt'.format(run), 'r')\n",
    "    \n",
    "    for line in file:\n",
    "        if line.split(':')[0] == 'ACCEPT':\n",
    "            data.append(line.split(':')[-1].strip())\n",
    "        elif line.split(':')[0] == 'PATH':\n",
    "            data.append(line.split(':')[-1].strip())\n",
    "        elif line.split(':')[0] == 'PLAN':\n",
    "            data.append(line.split(':')[-1].strip())\n",
    "        elif line.strip()[-2:] == 'ms':\n",
    "            d = line.split('!')[1].strip()[:-2]\n",
    "            data.append(float(d))\n",
    "            \n",
    "    file.close()\n",
    "    \n",
    "    if len(data) == 0:\n",
    "        data = [None] * 8\n",
    "    \n",
    "    if len(data) == 4:\n",
    "        data += [None] * 4\n",
    "\n",
    "    return data\n",
    "            \n",
    "def add_plan_to_run_data(n_runs):\n",
    "    count = 0\n",
    "    for run in os.listdir(repo):    \n",
    "        # with open(repo+'/{}/{}.json'.format(run, run), 'r') as infile:\n",
    "        #     run_data = json.load(infile)\n",
    "\n",
    "        log_data = extract_plan_data('mc3')\n",
    "        print(log_data)\n",
    "        return\n",
    "    #     run_data['accept1'] = log_data[0]\n",
    "    #     run_data['path1'] = log_data[1]\n",
    "    #     run_data['plan1'] = log_data[2]\n",
    "    #     if (log_data[2] != None):\n",
    "    #         run_data['steps1'] = int(len(log_data[2].split('->')[:-1]))\n",
    "    #     else:\n",
    "    #         run_data['steps1'] = log_data[2]\n",
    "    #     run_data['latency1'] = log_data[3]\n",
    "    #     run_data['accept2'] = log_data[4]\n",
    "    #     run_data['path2'] = log_data[5]\n",
    "    #     run_data['plan2'] = log_data[6]\n",
    "    #     if (log_data[6] != None):\n",
    "    #         run_data['steps2'] = int(len(log_data[6].split('->')[:-1]))\n",
    "    #     else:\n",
    "    #         run_data['steps2'] = log_data[6]\n",
    "    #     run_data['latency2'] = log_data[7]\n",
    "\n",
    "    #     with open(repo+'/{}/{}.json'.format(run, run), 'w') as outfile: \n",
    "    #         json.dump(run_data, outfile)\n",
    "            \n",
    "    #     count += 1\n",
    "\n",
    "    # print(run_data)\n",
    "      \n",
    "    try:\n",
    "        assert count == n_runs\n",
    "    except AssertionError:\n",
    "        print('FAILED: count = {}'.format(count))\n",
    "    \n",
    "add_plan_to_run_data(n_runs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7575676-10e6-49b6-ae74-1d86b11e5eb5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model checking resource usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3df9bc5-3d23-4efa-92a8-87e50dd69a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_stack_data(data):\n",
    "    result = []\n",
    "    \n",
    "    for item in data:\n",
    "        if item[:3] != 'SET':\n",
    "            result.append(item.strip())\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    result = result[2:-2]\n",
    "    # get stack size data here\n",
    "    result = [int(x[-1]) for x in result]\n",
    "            \n",
    "    return result\n",
    "    \n",
    "def _extract_set_data(data):\n",
    "    result = []\n",
    "    \n",
    "    for item in reversed(data[:-5]):\n",
    "        if item[:3] != 'SET':\n",
    "            try:\n",
    "                result.append(int(item.strip()))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    return result\n",
    "    \n",
    "def extract_performance_data(run):\n",
    "    perf = []\n",
    "    \n",
    "    for file in os.listdir(repo+'\\{}'.format(run)):\n",
    "        if file[:3] == 'dfs':\n",
    "            with open(repo+'\\{}\\{}'.format(run, file)) as infile:\n",
    "                perf.append(infile.readlines())\n",
    "         \n",
    "    set_data = []\n",
    "    stack_data = []\n",
    "    \n",
    "    for data in perf:\n",
    "        stack_data.append(max(_extract_stack_data(data)))\n",
    "        set_data.append(max(_extract_set_data(data)))\n",
    "    \n",
    "    return stack_data, set_data\n",
    "\n",
    "def add_performance_to_run_data(n_runs):\n",
    "    count = 0\n",
    "    for run in os.listdir(repo):    \n",
    "        with open(repo+'/{}/{}.json'.format(run, run), 'r') as infile:\n",
    "            run_data = json.load(infile)\n",
    " \n",
    "        stack_data, set_data = extract_performance_data(run)\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        df['stack_data'] = stack_data\n",
    "        df['set_data'] = set_data\n",
    "        df['state_size'] = 4\n",
    "        df['n_states'] = 15\n",
    "        df['adj_list_compile'] = 184\n",
    "        df['stack_compile'] = 12\n",
    "        df['set_compile'] = 24\n",
    "        df.to_csv(repo+'/{}/{}model_check.csv'.format(run, run))\n",
    "        \n",
    "        # if len(stack_data) > 0:\n",
    "        #     # max stack size and number of steps\n",
    "        #     print(stack_data)\n",
    "        #     run_data['max_stack_capacity'] = max(stack_data)\n",
    "        #     run_data['min_stack_size'] = min(stack_data)\n",
    "        #     run_data['mode_stack_size'] = statistics.mode(stack_data)\n",
    "        # else:\n",
    "        #     run_data['max_stack_capacity'] = None\n",
    "        #     run_data['min_stack_size'] = None\n",
    "        #     run_data['mode_stack_size'] = None\n",
    "            \n",
    "        # if len(set_data) > 0:\n",
    "        #     run_data['max_set_size'] = max(set_data)\n",
    "        #     run_data['min_set_size'] = min(set_data)\n",
    "        #     run_data['mode_set_size'] = statistics.mode(set_data)\n",
    "        # else:\n",
    "        #     run_data['max_set_size'] = None\n",
    "        #     run_data['min_set_size'] = None\n",
    "        #     run_data['mode_set_size'] = None\n",
    "            \n",
    "        # with open(repo+'/{}/{}.json'.format(run, run), 'w') as outfile: \n",
    "        #     json.dump(run_data, outfile)\n",
    "            \n",
    "        count += 1\n",
    "      \n",
    "    try:\n",
    "        assert count == n_runs\n",
    "    except AssertionError:\n",
    "        print('FAILED: count = {}'.format(count))\n",
    "\n",
    "add_performance_to_run_data(n_runs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124f0215-731d-44b2-bc2b-3e6739869c22",
   "metadata": {},
   "source": [
    "# Process resource usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72e69058-185e-4039-a558-3621d9b2001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_process_memory(run):\n",
    "    swap = []\n",
    "    physical = []\n",
    "    process = []\n",
    "    \n",
    "    file = open(repo+'/{}/usage.txt'.format(run), 'r')\n",
    "    \n",
    "    for line in file:\n",
    "        if line.split(':')[0] == 'MiB Mem ':\n",
    "            physical.append(line.split(':')[-1].strip())\n",
    "        elif line.split(':')[0] == 'MiB Swap':\n",
    "            swap.append(line.split(':')[-1].strip())\n",
    "        elif line.strip()[-8:] == 'autoctrl':\n",
    "            process.append(line)\n",
    "\n",
    "    file.close()\n",
    "\n",
    "    # physical data\n",
    "    physical_ = []\n",
    "    physical = [x.split(',') for x in physical]\n",
    "    for row in physical:\n",
    "        physical_.append([float(x.strip()[:4]) for x in row])\n",
    "\n",
    "    # swap data\n",
    "    swap_ = []\n",
    "    swap = [x.split(',') for x in swap]\n",
    "    for row in swap:   \n",
    "        swap_.append([x.strip() for x in row])\n",
    "\n",
    "    swap__ = []\n",
    "    for row in swap_:\n",
    "        swap__.append([float(x.split()[0]) for x in row])\n",
    "\n",
    "    # process data\n",
    "    process_ = []\n",
    "    for row in process:   \n",
    "        process_.append(row.strip()[9:-8].strip().split(' '))\n",
    "\n",
    "    process__ = []\n",
    "    for row in process_:\n",
    "        process__.append([x for x in row if x != ''])\n",
    "\n",
    "    for i in range(len(process__)):    \n",
    "        for j in range(len(process__[i])):\n",
    "            if ':' in process__[i][j]:\n",
    "                process__[i][j] = process__[i][j][-5:]\n",
    "                \n",
    "            try:\n",
    "                process__[i][j] = float(process__[i][j])\n",
    "            except ValueError:\n",
    "                pass\n",
    "                \n",
    "    phy_cols = ['mem_total', 'mem_free', 'mem_used', 'cache']\n",
    "    phy = pd.DataFrame(data=physical_, columns=phy_cols)\n",
    "\n",
    "    swp_cols = ['swap_total', 'swap_free', 'swap_used']\n",
    "    swp = pd.DataFrame(data=swap__, columns=swp_cols)\n",
    "\n",
    "    proc_cols = ['pr', 'ni', 'virt', 'res', 'shr', 'status', \n",
    "                 'cpu_perc', 'mem_perc', 'cpu_time']\n",
    "    proc = pd.DataFrame(data=process__, columns=proc_cols)\n",
    "\n",
    "    df = pd.merge(proc, phy, how='inner', left_index=True, right_index=True)\n",
    "    df = pd.merge(df, swp, how='inner', left_index=True, right_index=True)\n",
    "    df['process_uptime'] = 0.001 * df.index\n",
    "    df = df[df.mem_perc > 0]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_process_to_run_data(n_runs):\n",
    "    count = 0\n",
    "    for run in os.listdir(repo):    \n",
    "        with open(repo+'/{}/{}.json'.format(run, run), 'r') as infile:\n",
    "            run_data = json.load(infile)\n",
    "\n",
    "        df = _extract_process_memory(run)\n",
    "        df.to_csv(repo+'/{}/{}usage.csv'.format(run, run))\n",
    "\n",
    "        count += 1\n",
    "    try:\n",
    "        assert count == n_runs\n",
    "    except AssertionError:\n",
    "        print('FAILED: count = {}'.format(count))\n",
    "\n",
    "add_process_to_run_data(n_runs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e95b95-0ea5-46cf-86c7-046c6703a485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
